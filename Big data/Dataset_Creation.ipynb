{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md1_BKOheO8I"
      },
      "source": [
        "\n",
        "**Note**: you may get an error when installing hopsworks on Colab, and it is safe to ignore it.\n",
        "\n",
        "This is the second part of the quick start series of tutorials about predicting customers that are at risk of churning with the Hopsworks Feature Store. This notebook explains how to read from a feature group and create training dataset within the feature store\n",
        "\n",
        "## 🗒️ In this notebook you will see how to create a training dataset from the feature groups:\n",
        "1. **Select the features** you want to train the model on,\n",
        "2. **How the features should be preprocessed,**\n",
        "3. **Create a dataset split** for training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC6Gm2XdeO8N",
        "outputId": "4cb89e1e-e30f-4f6b-a8c3-b88ca3b3608e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.2/68.2 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.6/133.6 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.4/519.4 KB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hopsworks (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for avro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hsml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for hsfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for twofish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for thrift (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyhopshive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "werkzeug 2.2.3 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.\n",
            "pandas-profiling 3.2.0 requires markupsafe~=2.1.1, but you have markupsafe 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U hopsworks --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLY8dxjTeO8O",
        "outputId": "50ffb84f-e11e-4a68-ec9d-373f66582db8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected. Call `.close()` to terminate connection gracefully.\n",
            "\n",
            "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/28868\n",
            "Connected. Call `.close()` to terminate connection gracefully.\n"
          ]
        }
      ],
      "source": [
        "import hopsworks\n",
        "\n",
        "project = hopsworks.login(api_key_value=\"<api key>\")\n",
        "\n",
        "fs = project.get_feature_store()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuTXtoL1eO8O"
      },
      "source": [
        "---\n",
        "## Feature Selection\n",
        "\n",
        "You will start by selecting all the features you want to include for model training/inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1FCnUCx6eO8P"
      },
      "outputs": [],
      "source": [
        "# Load feature groups.\n",
        "customer_info_fg = fs.get_feature_group(\n",
        "    name=\"customer_info\",\n",
        "    version=1\n",
        ")\n",
        "\n",
        "demography_fg = fs.get_feature_group(\n",
        "    name=\"customer_demography_info\",\n",
        "    version=1\n",
        ")\n",
        "\n",
        "subscriptions_fg = fs.get_feature_group(\n",
        "    name=\"customer_subscription_info\",\n",
        "    version=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RYIq2wu_eO8P"
      },
      "outputs": [],
      "source": [
        "# Select features for training data.\n",
        "ds_query = customer_info_fg.select_except([\"customerid\"]).join(demography_fg.select_except([\"customerid\"])).join(subscriptions_fg.select_all())\n",
        "\n",
        "# uncomment this if you would like to view query result\n",
        "# ds_query.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2T6tJKtfV01",
        "outputId": "1b1ba057-03fb-484b-cda3-11e0af4f1013"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "hsfs.constructor.query.Query"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(ds_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBU2WteQeO8P"
      },
      "source": [
        "Recall that you created three feature groups in the previous notebook. If you had created multiple feature groups with identical schema and wanted to include them in the join you would need to include a prefix argument in the join to avoid feature name clash. See the [documentation](https://docs.hopsworks.ai/feature-store-api/latest/generated/api/query_api/#join) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xPrtuE4eO8Q"
      },
      "source": [
        "---\n",
        "## Transformation Functions\n",
        "\n",
        "You will preprocess the data using *min-max scaling* on numerical features and *label encoding* on categorical features. To do this you will simply define a mapping between features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xAoHKgEaeO8R"
      },
      "outputs": [],
      "source": [
        "# Load transformation functions.\n",
        "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
        "label_encoder = fs.get_transformation_function(name=\"label_encoder\")\n",
        "\n",
        "numerical_features = [\"tenure\", \"monthlycharges\", \"totalcharges\"]\n",
        "categorical_features = [\n",
        "    \"multiplelines\", \"internetservice\", \"onlinesecurity\", \"onlinebackup\",\n",
        "    \"deviceprotection\", \"techsupport\", \"streamingmovies\", \"streamingtv\",\n",
        "    \"phoneservice\", \"paperlessbilling\", \"contract\", \"paymentmethod\", \"gender\", \n",
        "    \"dependents\", \"partner\"\n",
        "]\n",
        "\n",
        "# Map features to transformations.\n",
        "transformation_functions = {}\n",
        "for feature in numerical_features:\n",
        "    transformation_functions[feature] = min_max_scaler\n",
        "\n",
        "for feature in categorical_features:\n",
        "    transformation_functions[feature] = label_encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeGNivyQeO8R"
      },
      "source": [
        "---\n",
        "## Feature View Creation\n",
        "\n",
        "The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions.\n",
        "In order to create a Feature View you may use `fs.create_feature_view()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMY81YdSeO8S",
        "outputId": "8bc864a8-f37a-4376-d012-8d784891caed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature view created successfully, explore it at \n",
            "https://c.app.hopsworks.ai:443/p/28868/fs/28788/fv/churn_feature_view/version/1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    feature_view = fs.get_feature_view(name = 'churn_feature_view', version = 1)\n",
        "except:\n",
        "    feature_view = fs.create_feature_view(\n",
        "        name = 'churn_feature_view',\n",
        "        version = 1,\n",
        "        labels=[\"churn\"],\n",
        "        transformation_functions=transformation_functions,\n",
        "        query=ds_query,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wkq2wp-eO8S"
      },
      "source": [
        "---\n",
        "## Training Dataset Creation\n",
        "\n",
        "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
        "\n",
        "**Training Dataset  may contain splits such as:** \n",
        "* Training set - the subset of training data used to train a model.\n",
        "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
        "* Test set - the holdout subset of training data used to evaluate a mode\n",
        "\n",
        "Training dataset is created using `fs.create_train_validation_test_split()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72Nc0aaweO8S",
        "outputId": "5ced6284-f95a-4d87-c30c-83f631648255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training dataset job started successfully, you can follow the progress at \n",
            "https://c.app.hopsworks.ai/p/28868/jobs/named/churn_feature_view_1_1_create_fv_td_31032023063929/executions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VersionWarning: Incremented version to `1`.\n"
          ]
        }
      ],
      "source": [
        "td_version, td_job = feature_view.create_train_validation_test_split(\n",
        "    description = 'churn_training_dataset_random_splitted',\n",
        "    data_format = 'csv',\n",
        "    validation_size = 0.2,\n",
        "    test_size = 0.1,\n",
        "    write_options = {'wait_for_job': True},\n",
        "    coalesce = True,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
